\section{Introduction}
As team we had first decided on a different topic "Will I live longer if I cycle to work?" . But in the process of trying to collect data it was seen that there was a lot of scientific studies associated with the topic but we struggled to find a way of meaningfully combining different life expectancy models.  Most of the data we found was already clean and we did not have much processing to do\cite{iacono2008access}. We could not find out any scientific method that directly links the factors we were considering(amount of oxygen intake, pollution,BMI etc)to the increase in life expectancy\cite{edwards2014spinning}.Combining all these factors into one prediction model became a challenge.
\newline
So ,after spending almost 4 weeks in trying to collect data we had to shift our focus to some other topic where we could get a significantly high volume of data.  Now we are trying to predict the top three indicators affecting the currency of that country.  After a considerable amount of research We have taken into account the following factors exchange rates, interest rates, employment, population, import, export, GDP and inflation\cite{kuruwitaarachchi2018design}.  The annual factors, like the employment, population, trade networks, GDP and inflation are taken in the time range of 1991-2016.

% Head 1
\section{Problem Statement}
\subsection{High Frequency Exchange Rate Data}
Our aim was to investigate how exchange rates influence each other.  As we were able to gather nearly a 1000 samples we where able to perform a number of different machine learning techniques to perform this analysis.

\subsection{Low Frequency Economic Indicators}
From investigating economic research we were directed towards a number of different economic indicators which are understood to influence a currencies value.  These indicators are only reported on a annual basis and due to a number of factors was only available from 1999 to 2015.  This means we only had 17 samples.  This limited the type of analysis we where able to perform. So we decided to find the economic which where most correlated to the performance of a specific exchange rate. 

\section{Implementation}
\subsection{High Frequency Exchange Rate Data}
\subsubsection{}{Cleaning with python}
\newline
The data that we have collected is from oanda.com, Bank of England and World Bank websites. 
\newline

\iffalse
\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.40\textwidth]{aa.png}
		\caption{}
		\label{}
	\end{center}
\end{figure}
\fi

First step, Download : For the currency exchange data, it was about weekly average exchange rates from December 1998 to December 2017. We considered all currency of the G20's countries and downloaded all data considering the GBP as base currency\cite{abbate2018point}. Every file downloaded was made in this way (see Fig\ref{raw_curr}): 

\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.40\textwidth]{aaa2.png}
		\caption{Raw currency data}
		\label{raw_curr}
	\end{center}
\end{figure}


For currency exchange data below technique was observed
\begin{itemize} 
\item Size of table : 1240 x 4 columns 
\item Col[0]= Week(1,2,3,4) 
\item Col[1]= Month-Year(i.e may 2002) 
\item Col[2]= Bid ( Bid is the price a buyer is willing to pay for a security) 
\item Col[3]= Ask ( Ask is the price a seller is willing to accept for a security) 
\end{itemize}

Second step, Aggregation : We had 16 files(16 because in the G20 group France, Italy, Germany and European Union has currency Euro).  Using python we gave in input a file formed by 1240(included blank space between rows) x 4 columns and received the output below in Fig \ref{part_proccessed}. 
\newline

\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.40\textwidth]{aaa1.png}
		\caption{Intermediate Processed Data}
		\label{part_proccessed}
	\end{center}
\end{figure}

All spaces were removed using python,  saved in one different file, and an average between bid and ask was calculated.
\newline
Third step, Creation of a complete matrix : We wrote another python script called complete Matrix.py that was used to create all possible currency pair. Remembering that the currency were 16 , we had a output matrix with 16x16 column and 994 rows (see Fig\ref{extrap_ex_data}).

\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.4\textwidth]{bbb.png}
		\caption{Extrapolated exchange rate data}
		\label{extrap_ex_data}
	\end{center}
\end{figure}

\subsubsection{Modeling}
The target for this assignment was to find, giving in input one currency exchange, the \textbf{three} most relevant currency exchanges that most influences the input. To do that we considered the matrix containing all possible combinations of currency exchanges and we used Matlab for the analysis. We used linear regression technique to create a model to find if there is a clear relationship between one exchange rate and the other G20 exchange rates. We then look to find what are the top 3 currencies that influence a specific exchange rate.
\newline
Procedure:
To find the most relevant exchange rates for the input, it was necessary to write a Matlab script using CVX tool. CVX is a modeling system for constructing and solving disciplined convex programs (DCPs)\cite{leung2000forecasting}. 

\includegraphics[width=0.4\textwidth]{ccc.png}

Explanation of the procedure : 

\begin{itemize}
\item cvx\_begin : Must be written as the first instruction of a CVX model
\item cvx\_begin quiet : Prevents the model from producing any screen output while it is being solved.
\item cvx\_end : Must be the last instruction of the CVX procedure 
\item variable : It is used to declare the variable, it includes the name of the variable, an optional dimension list, and one or more keywords that provide additional information about the content or structure of the variable.
\item minimise : It is a command used to declare an objective function (can be also maximise) 
N.B. The objective function in a call to minimise must be convex; the objective function in a call to maximise must be concave. In this case Y*w2-f is convex.
\newline
\textbf{Goal :To find the best weight vector that minimises the error }
\item +gamma(w2) : We used gamma for the L1 regularisation like \textit{\textbf{Lasso}}. This technique is normally used to solve the overfitting problem in statistical models. \textbf{In this particular case we thought that it was a good idea since we built a model using 211 different variables and so the model was complex and the risk of overfitting was high}
\item find abs(w2)\textgreater(1e-5) : we found which weights are not switched off by the regulariser, that are the most relevant variables. 
\end{itemize}

After obtaining the most relevant values, we split all the data in training set and test set and calculated the MSE (Means Squared Error) on the test set. After executing the script 15 times ,we had an average error of 20 on the test set\cite{rossi2013exchange}. 

Selection of gamma value:
\newline
One of the most difficult thing was about setting gamma for the regularisation. After a considerable amount of research we understood that it is almost impossible set a good gamma because it is totally dependent on both the training set and all parameters that were used\cite{philip2011artificial}. 

\begin{itemize}
\item Gamma is dependent on both the training set and the other parameters you use.
\item There is no “good Gamma” for any data set alone
\item Mathematically you call “Gamma” the “Lagrangian multiplier” (complexity control).
\item The higher Gamma is, the higher the regularisation. Increasing Gamma results in less overfitting but also greater bias.
\item Gamma values around 20 are extremely high, and should be used only when you are using high depth or if you want to directly control the features which are dominating in the data set (i.e too strong feature engineering). 

\end{itemize}

To find the most relevant values, based on the dataset, gamma was changed automatically in every iteration, and so it was increased until exactly three variables were not switched off.

Create a model:
\newline
The second analysis that we did using the high frequency data values, was about creating a model that was able to derive the currency exchange trend establishing the top three influential currency exchange. To do that we still used the CVX tool, regularising the function but in this case gamma was fixed. We made researches and we saw many examples, moreover we run practical testing with different values of gamma and at the end we decided to set gamma = 8.0.

The dataset was divided in training and test using the proportion 75\% and 25\% respectively.  Then the script was run using CVX tool. 

\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.4\textwidth]{error.png}
		\caption{Error Calculation}
		\label{error_calc}
	\end{center}
\end{figure}

\iffalse
In the application we put two tables containing the top 3 most and the top 3 least predictable exchange rates, the currencies exchanges with the lowest error and the highest error on the test set respectively.
\fi

Justification of the model:
\newline
In the beginning there were some doubts about using regression technique or an ANN(Artificial Neural network)\cite{leung2000forecasting}.
We considered both merits and de-merits but in the end we chose to use linear regression principally for two reasons : 
\begin{itemize}
\item The main reason was that the ANN is a black box method and it is very difficult to find any relationship between variables, on the contrary these relationships can easily be shown by regression models. 
\item The method of least squared regression converge much faster than a neural network, and this means a saving of resources and time 
\end{itemize}

\subsection{Low Frequency data}
The low frequency data included the data of G-20 population, employment, trade network, GDP, inflation and interest rate. Every files downloaded was made in this way (see Fig \ref{raw_eco_data}): 
\newline

\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.40\textwidth]{all_data.png}
		\caption{Raw economic indicators data}
		\label{raw_eco_data}
	\end{center}
\end{figure}


The size of each factor of G-20 dataset table is 265 rows time 61 columns and each row is a country's factor data  from 1960 to 2016.
Firstly, we used python code to select the data of G-20 and used append function to put the G-20 data into a new array.
Then,  in order to clean out the data suitable for using and analysis, we used the zip function to transpose the array.
Next, Due to the low frequency data were analysed from 1991 to 2016, we filtered the data and put it into the new csv file.
Furthermore, we used replace function to remove the space, which may cause the Index error.  Using this python script all low frequency data were filtered and we put the all cleaned data into one file (see Fig \ref{cleaned_eco_data}.
\newline

\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.40\textwidth]{Capture.JPG}
		\caption{Cleaned Economic data}
		\label{cleaned_eco_data}
	\end{center}
\end{figure}

\subsection{Web Application Design}
\subsubsection{}{Framework}
We created a simple web application using the python flask framework.
\subsubsection{}{Displaying Result}
The final analysis and displaying of results was implemented in JavaScript and D3.  

The when viewing the low frequency economic indicator data we also load the high frequency (weekly samples) exchange rate data, this is then aggregated down to yearly samples for analysis with the economic indicators.
\newline
 
\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=0.30\textwidth]{web.jpg}
		\caption{Covariance Scatter plot of Economic Indicators with respect to a specific exchange rate}
		\label{scatter_plot}
	\end{center}
\end{figure}

The web app has the following two functions:
\newline
1)	Finding Strongest Correlation :When this button is pressed the web app iterates through each exchange rate and calculates the correlation coefficient between it and each related economic indicator.  This sorted in order of strength (closest to either 1.0 or -1.0) and then displayed.  This allows us to find what are the strongest and weakest correlations. The app then also aggregates the absolute value of each correlation coefficient (to prevent positive and negative correlations cancelling each other out and allowing us to find the average strength of the correlation) with respect to the type of economic indicator.  This then allows us to find the type of economic indicator which is most correlated with exchange rates.
\newline
2)	Go!:This button takes a selected exchange rate and then generates a scatter plot for each of the economic indicators that are most correlated with that exchange rate. These are present in a 4 x 4 grid (as seen in Fig \ref{scatter_plot}, the selected exchange rate is in the top right corner, and each economic indicator is plotted on the diagonal, each labelled with their respective correlation coefficient related to the exchange rate.  The user is then able to see a covariance scatter plot for each variable by tracing the horizontal and vertical intersection.

\section{Results}
\subsection{Observation}
As the data we have used is not very huge, we chose to find the strongest correlation between the exchange rates and the indicators which led to this correlation. We have used Pearson's correlation coefficient as the mathematical approach since the data was not suitable for any mathematical regression or any other kind of predictive analysis. The result that we have found can be summarised below showing the ranking of the indicators in terms of those that most strongly correlate to exchange rate data averaged out over all exchange rates.
\newline
\begin{itemize}
\item Population = 0.62
\item GDP = 0.60
\item Imports = 0.59
\item Exports = 0.58
\item Foreign Trade = 0.50
\item Inflation GDP Deflator = 0.40
\item Employment = 0.40
\item Interest Rate = 0.40
\item Inflation Consumer Prices = 0.38
\end{itemize}

However in some particular cases we have seen a very strong correlation which are worth mentioning
\begin{table}[!h]
\centering
\caption{Strongest correlation and related indicator}
\label{my-label}
\begin{tabular}{|l|l|l|}
\hline
Currency Code  & Indicator       & Value \\ \hline
JPY-USD        & JPY GDP         & 0.99  \\ \hline
JPY-SAR        & JPY GDP         & 0.99  \\ \hline
SAR-JPY        & JPY GDP         & -0.99 \\ \hline
MXN-CNY        & MXN Population  & -0.98 \\ \hline
AUD-TRY        & TRY Population  & 0.98  \\ \hline
CAD-TRY        & TRY Population  & 0.98  \\ \hline
SAR-CNY        & SAR imports     & -0.98 \\ \hline
AUD-IDR        & IDR Population  & 0.98  \\ \hline
AUD-TRY        & AUD Population  & 0.98  \\ \hline
MXN-CNY        & CNY Population  & -0.98 \\ \hline
RUB-CAD        & CAD Population  & -0.98 \\ \hline
CAD-TRY        & CAD Population  & 0.98  \\ \hline
CNY-SAR        & SAR Imports     & 0.97  \\ \hline
TRY-MXN        & MXN Inflation   & 0.97  \\ \hline
RUB-AUD        & AUD Population  & -0.97 \\ \hline
AUD-IDR        & AUD Population  & 0.97  \\ \hline
AUD-ZAR        & AUD Population  & 0.97  \\ \hline
AUD-ZAR        & ZAR Population & 0.97  \\ \hline
\end{tabular}
\end{table}

From the table above, a few things can be concluded

\begin{itemize}
\item GDP, Population, Imports and Inflation have been the strongest indicators in the past\cite{edwards2006relationship}. These factors have played the most important role in determining the exchange rates for the two currencies
\item A high correlation value indicates that the corresponding indicator is the strongest factor in determining the exchange rates between those two countries\cite{burstein2005large}. For example, We find Japanese GDB has a very strong correlation value (0.99) between Japanese Yen and US Dollar and also between Japanese Yen and South African Rand.  This dose not necessarily mean there is a strong correlation, to analysis this we must then look at the covariance scatter plot to understand the relationship. In this case we observe a strong correlation.
\end{itemize}
\subsection{Analysis}
\subsubsection{}{Currency Exchange Trends}
If we look at the currency exchange rates between Argentina (ARS) and USA (USD) we observe that until 2001 the currency exchange value is 1 and then we see a sudden dip in the value. What could be the reason for this dip? Well, that was because each peso was index-linked to USD at 1ARS= 1USD. However, after the financial crisis of 2001, the fixed exchange rate system was abandoned. Since 2002, the exchange rate started to fluctuate, keeping the exchange rate at between 2.90 and 3.10 pesos per US dollar at that time.  
This is the same case in terms of Saudi Riyal, where even today it is index linked to the USD @ 1 USD = 3.75 SAR 
\subsubsection{}{Correlation Coefficient}
If we look at the currency exchange rates JPY-USD, we observe that it has a .99 correlation value against the GDP of Japan. Yes, without any arguments we can agree that the GDP of a country has a direct impact on its currency value. Where we see that the overall influence (correlation coefficient value) of GDP to its countries currency is calculated to be .6. But in this case, we see a value of .99.
\newline
If we look at the history for the currency of Japan (YEN) we see that. Following World War II the Yen lost much of its value. To stabilise the Japanese economy the exchange rate of the yen was fixed at ¥360 per 1USD as part of the Bretton Woods system. When that system was abandoned in 1971, the Yen became undervalued and was allowed to float. The Yen had appreciated to a peak of ¥271 per 1 USD in 1973, then underwent periods of depreciation and appreciation due to the 1973 oil crisis, arriving at a value of ¥227 per 1 USD by 1980. Since 1973, the Japanese government has maintained a policy of currency intervention, and the yen is therefore under a "dirty float" regime. This intervention continues until today and that is the reason we see such a tight correlation between the currency exchange of JPY-USD against the GDP of Japan.\newline
But the question is does it stand good for other cases? Well, yes it does? For instance, if we look at the currency of China(CNY) and USA(USD) we see that both these two countries have a relatively huge GDP values, which does have an impact on their respective currencies. But when we look at the exchange rate which is a copula of both these countries currency. There is a possibility that the influence of the GDP values on the exchange rate tend to be slightly lower (Correlation coefficient of China’s GDP and US GDP against the exchange rate of CNY-USD is .92 and .87 respectively) but still have a significant impact the exchange rates. 

\section{Conclusion}



     







\section{Limitations and future work}

\begin{itemize}
	\item Our application works on static data .If we want to analyse new data, we need to upload new file into the application.
	\item Most of the currencies were index linked to USD prior to 1999, for which we had a limited availability of currency data.
	\item
\end{itemize}



 



% Start of "Sample References" section

% Appendix

% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-bibliography}
